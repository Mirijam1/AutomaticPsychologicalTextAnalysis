{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "This notebook contains the methods that are commonly used between the peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mirij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import gensim.downloader as api\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = RegexpTokenizer(r'\\w+')\n",
    "schemas = [\"vulnerable\", \"angry\", \"impulsive\", \"happy\", \"detached\", \"punishing\", \"healthy\"]\n",
    "num_of_schemas = 7\n",
    "max_words = 2000\n",
    "max_epochs = 30\n",
    "vec_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get binary labels\n",
    "Return labels 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in dataframe, returns list of 'Texts' and list of 'Labels'\n",
    "def get_text_labels(dataframe):\n",
    "    rows, cols = (dataframe.shape[0], dataframe.shape[1])\n",
    "\n",
    "    text_list = []\n",
    "    label_list = np.zeros((rows, len(schemas)))\n",
    "\n",
    "    texts = dataframe['Text']\n",
    "    for txt in texts:\n",
    "        text_list.append(txt)\n",
    "\n",
    "    is_vulnerable = dataframe['is_vulnerable']\n",
    "    is_angry = dataframe['is_angry']\n",
    "    is_impulsive = dataframe['is_impulsive']\n",
    "    is_happy = dataframe['is_happy']\n",
    "    is_detached = dataframe['is_detached']\n",
    "    is_punishing = dataframe['is_punishing']\n",
    "    is_healthy = dataframe['is_healthy']\n",
    "\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        j = 0\n",
    "        label_list[i][j] = 1 if bool(is_vulnerable[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_angry[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_impulsive[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_happy[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_detached[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_punishing[i]) == True else 0\n",
    "        j += 1\n",
    "        label_list[i][j] = 1 if bool(is_healthy[i]) == True else 0\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ordinal labels\n",
    "Return labels from 0-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_for_each_label(dataframe):\n",
    "    rows, cols = (dataframe.shape[0], dataframe.shape[1])\n",
    "    text_list = []\n",
    "\n",
    "    texts = dataframe['Text']\n",
    "    for txt in texts:\n",
    "        text_list.append(txt)\n",
    "\n",
    "    average_label_list = np.zeros((rows, len(schemas)))\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        j = 0\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 5, 15)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 16, 26)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 27, 35)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 36, 46)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 47, 56)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 57, 67)\n",
    "        j += 1\n",
    "        average_label_list[i][j] = avg_helper(dataframe, i, 68, 78)\n",
    "\n",
    "    return text_list, average_label_list\n",
    "\n",
    "\n",
    "def avg_helper(dataframe, i, begin, end):\n",
    "    mean = dataframe.iloc[i, begin:end].mean()\n",
    "    for j in dataframe.iloc[i, begin:end]:\n",
    "        if (j is 5 or j is 6) and mean < 3.5:\n",
    "            mean = 3.5\n",
    "    return get_label(mean)\n",
    "\n",
    "\n",
    "def get_label(mean) -> int:\n",
    "    mean = round(mean)\n",
    "    if mean <= 3:\n",
    "        return 0\n",
    "    elif 3 < mean <= 4:\n",
    "        return 1\n",
    "    elif 4 < mean <= 5:\n",
    "        return 2\n",
    "    elif 5 < mean <= 6:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "Splits the dataset into a training and test set. Given percentage is the size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(input_x, label_y, percent: float) -> (list, list, list, list):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(input_x, label_y, test_size=percent)\n",
    "    x_train, y_train, x_test, y_test = iterative_train_test_split(input_x, label_y, test_size=percent)\n",
    "    return x_train, y_train, x_test, y_test, percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "Clean dataset by:\n",
    "- Lowercasing text\n",
    "- Expanding contractions\n",
    "- Removing stopwords\n",
    "- Lemmatization\n",
    "\n",
    "Also returns the tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(texts: list) -> list:\n",
    "    tokenized_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        words = tk.tokenize(texts[i])\n",
    "        tokenized_texts.append(words)\n",
    "    return tokenized_texts\n",
    "\n",
    "\n",
    "def remove_stopwords(s: str) -> str:\n",
    "    new_str = \"\"\n",
    "    for word in s.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_str += word + \" \"\n",
    "    return new_str\n",
    "\n",
    "\n",
    "# Return list of tokenized strings through pre-processing(lowercase, noise removal, stop-word removal)\n",
    "def pre_process_data(texts: list) -> (list, list):\n",
    "    # Convert all to lowercase\n",
    "    processed_texts = list(map(lambda s: s.lower(), texts))\n",
    "    \n",
    "    # Noise removal\n",
    "    processed_texts = list(map(lambda s: contractions.fix(s), processed_texts))\n",
    "\n",
    "    # Stop word-removal\n",
    "    processed_texts = list(map(lambda s: remove_stopwords(s), processed_texts))\n",
    "\n",
    "    # Tokenizer of strings\n",
    "    tokenized_texts = tokenizer(processed_texts)\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_texts = list(map(lambda s: (list(map(lambda y: lemmatizer.lemmatize(y), s))), tokenized_texts))\n",
    "    processed_texts = list(map(lambda s: ' '.join(list(map(lambda y: lemmatizer.lemmatize(y), s))), tokenized_texts))\n",
    "\n",
    "    return processed_texts, tokenized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Load Word2Vec word vectors that are pre-trained by Mikolov on 100 billion words of Google News. The word vectors are trained with the Continuous Bag-Of-Words (CBOW) model. Each vector has a dimensionality of 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns pre-trained word2vec model\n",
    "def get_word2vec():\n",
    "    print('LOAD PRE-TRAINED WORD2VEC')\n",
    "\n",
    "    return api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(input_list):\n",
    "    i = 0\n",
    "    result = []\n",
    "    for sentence in input_list:\n",
    "        result.append(gensim.models.doc2vec.TaggedDocument(sentence, [i]))\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "def training_model_d2v(data=None):\n",
    "    # train d2v on own data\n",
    "    # taggedDocs = read_corpus(data)\n",
    "    # print(\"TRAINING MODEL\")\n",
    "    #\n",
    "    # model = gensim.models.Doc2Vec( documents=taggedDocs, vector_size=vec_size, window=10, epochs=max_epochs, min_count=1, workers=4, alpha=0.025, min_alpha=0.025)\n",
    "    # model.save(\"../model/schema-d2v.model2\")\n",
    "\n",
    "    # load pre-trained data\n",
    "    print(\"LOAD PRE-TRAINED DOC2VEC\")\n",
    "    model = gensim.models.Doc2Vec.load(\"../model/apnews_dbow.tgz\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_fast_text():\n",
    "    # If model is obtained, no need to run this part of code\n",
    "    # fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "    ft = fasttext.load_model('cc.en.300.bin')\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}